{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df326eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a5812",
   "metadata": {},
   "source": [
    "# Downloading\n",
    "> Fastkafka app for Downloading infobip stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from os import environ\n",
    "import re\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from typing import Union, Optional, Tuple, Dict, Any\n",
    "from contextlib import contextmanager\n",
    "from urllib.parse import quote_plus as urlquote\n",
    "from urllib.parse import unquote_plus as urlunquote\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sqlalchemy.engine import Connection, create_engine\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20411862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from infobip_kafka_service.logger import get_logger\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbcd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _create_clickhouse_connection_string(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    database: str,\n",
    "    protocol: str,\n",
    ") -> str:\n",
    "    # Double quoting is needed to fix a problem with special character '?' in password\n",
    "    quoted_password = urlquote(urlquote(password))\n",
    "    conn_str = (\n",
    "        f\"clickhouse+{protocol}://{username}:{quoted_password}@{host}:{port}/{database}\"\n",
    "    )\n",
    "\n",
    "    return conn_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f03176",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = _create_clickhouse_connection_string(\n",
    "    username=\"default\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=8123,\n",
    "    database=\"infobip\",\n",
    "    #     table=\"events\",\n",
    "    protocol=\"http\",\n",
    ")\n",
    "assert actual == \"clickhouse+http://default:123456@localhost:8123/infobip\"\n",
    "\n",
    "actual = _create_clickhouse_connection_string(\n",
    "    username=\"default\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=9000,\n",
    "    database=\"infobip\",\n",
    "    #     table=\"events\",\n",
    "    protocol=\"native\",\n",
    ")\n",
    "assert actual == \"clickhouse+native://default:123456@localhost:9000/infobip\"\n",
    "\n",
    "actual = _create_clickhouse_connection_string(\n",
    "    username=\"default\",\n",
    "    password=\"123?456@\",\n",
    "    host=\"localhost\",\n",
    "    port=9000,\n",
    "    database=\"infobip\",\n",
    "    #     table=\"events\",\n",
    "    protocol=\"native\",\n",
    ")\n",
    "assert (\n",
    "    actual == \"clickhouse+native://default:123%253F456%2540@localhost:9000/infobip\"\n",
    "), actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e162c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def create_db_uri_for_clickhouse_datablob(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    table: str,\n",
    "    database: str,\n",
    "    protocol: str,\n",
    ") -> str:\n",
    "    \"\"\"Create uri for clickhouse datablob based on connection params\n",
    "\n",
    "    Args:\n",
    "        username: Username of clickhouse database\n",
    "        password: Password of clickhouse database\n",
    "        host: Host of clickhouse database\n",
    "        port: Port of clickhouse database\n",
    "        table: Table of clickhouse database\n",
    "        database: Database to use\n",
    "        protocol: Protocol to connect to clickhouse (native/http)\n",
    "\n",
    "    Returns:\n",
    "        An uri for the clickhouse datablob\n",
    "    \"\"\"\n",
    "    clickhouse_uri = _create_clickhouse_connection_string(\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        protocol=protocol,\n",
    "    )\n",
    "    clickhouse_uri = f\"{clickhouse_uri}/{table}\"\n",
    "    return clickhouse_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_test_cases = [\n",
    "    dict(\n",
    "        username=\"default\",\n",
    "        password=\"123456\",\n",
    "        host=\"localhost\",\n",
    "        port=9000,\n",
    "        database=\"infobip\",\n",
    "        table=\"events\",\n",
    "        protocol=\"native\",\n",
    "        db_uri=\"clickhouse+native://default:123456@localhost:9000/infobip/events\",\n",
    "    )\n",
    "]\n",
    "\n",
    "for test_case in db_test_cases:\n",
    "    actual_db_uri = create_db_uri_for_clickhouse_datablob(\n",
    "        username=test_case[\"username\"],\n",
    "        password=test_case[\"password\"],\n",
    "        host=test_case[\"host\"],\n",
    "        port=test_case[\"port\"],\n",
    "        table=test_case[\"table\"],\n",
    "        database=test_case[\"database\"],\n",
    "        protocol=test_case[\"protocol\"],\n",
    "    )\n",
    "    print(f\"{actual_db_uri=}\")\n",
    "    assert actual_db_uri == test_case[\"db_uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _get_clickhouse_connection_params_from_db_uri(\n",
    "    db_uri: str,\n",
    ") -> Tuple[str, str, str, int, str, str, str, str]:\n",
    "    \"\"\"\n",
    "    Function to get clickhouse connection params from db_uri of the db datablob\n",
    "\n",
    "    Args:\n",
    "        db_uri: DB uri of db datablob\n",
    "    Returns:\n",
    "        The username, password, host, port, table, database, protocol, database_server of the db datablob as a tuple\n",
    "    \"\"\"\n",
    "    result = re.search(\"(.*)\\+(.*):\\/\\/(.*):(.*)@(.*):(.*)\\/(.*)\\/(.*)\", db_uri)\n",
    "    database_server = result.group(1)  # type: ignore\n",
    "    protocol = result.group(2)  # type: ignore\n",
    "    username = result.group(3)  # type: ignore\n",
    "    password = urlunquote(urlunquote(result.group(4)))  # type: ignore\n",
    "    host = result.group(5)  # type: ignore\n",
    "    port = int(result.group(6))  # type: ignore\n",
    "    database = result.group(7)  # type: ignore\n",
    "    table = result.group(8)  # type: ignore\n",
    "    return username, password, host, port, table, database, protocol, database_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df549c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_case in db_test_cases:\n",
    "    (\n",
    "        actual_username,\n",
    "        actual_password,\n",
    "        actual_host,\n",
    "        actual_port,\n",
    "        actual_table,\n",
    "        actual_database,\n",
    "        actual_protocol,\n",
    "        actual_database_server,\n",
    "    ) = _get_clickhouse_connection_params_from_db_uri(db_uri=test_case[\"db_uri\"])\n",
    "    display(\n",
    "        f\"{actual_username=}\",\n",
    "        f\"{actual_password=}\",\n",
    "        f\"{actual_host=}\",\n",
    "        f\"{actual_port=}\",\n",
    "        f\"{actual_table=}\",\n",
    "        f\"{actual_database=}\",\n",
    "        f\"{actual_protocol=}\",\n",
    "        f\"{actual_database_server=}\",\n",
    "    )\n",
    "\n",
    "    assert actual_username == test_case[\"username\"]\n",
    "    assert actual_password == test_case[\"password\"]\n",
    "    assert actual_host == test_case[\"host\"]\n",
    "    assert actual_port == test_case[\"port\"]\n",
    "    assert actual_table == test_case[\"table\"]\n",
    "    assert actual_database == test_case[\"database\"]\n",
    "    assert actual_protocol == test_case[\"protocol\"]\n",
    "    assert actual_database_server == \"clickhouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c364e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def get_clickhouse_params_from_env_vars() -> Dict[str, Union[str, int]]:\n",
    "    return dict(\n",
    "        username=environ[\"KAFKA_CH_USERNAME\"],\n",
    "        password=environ[\"KAFKA_CH_PASSWORD\"],\n",
    "        host=environ[\"KAFKA_CH_HOST\"],\n",
    "        database=environ[\"KAFKA_CH_DATABASE\"],\n",
    "        port=int(environ[\"KAFKA_CH_PORT\"]),\n",
    "        protocol=environ[\"KAFKA_CH_PROTOCOL\"],\n",
    "        table=environ[\"KAFKA_CH_TABLE\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c988dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(get_clickhouse_params_from_env_vars().keys()) == set(\n",
    "    [\"database\", \"host\", \"password\", \"port\", \"protocol\", \"table\", \"username\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad53c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@contextmanager  # type: ignore\n",
    "def get_clickhouse_connection(  # type: ignore\n",
    "    *,\n",
    "    username: str,\n",
    "    password: str,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    database: str,\n",
    "    table: str,\n",
    "    protocol: str,\n",
    "    #     verbose: bool = False,\n",
    ") -> Connection:\n",
    "    if protocol != \"native\":\n",
    "        raise ValueError()\n",
    "    conn_str = _create_clickhouse_connection_string(\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        protocol=protocol,\n",
    "    )\n",
    "    \n",
    "#     print(f\"{conn_str=}\")\n",
    "\n",
    "    db_engine = create_engine(conn_str)\n",
    "    # args, kwargs = db_engine.dialect.create_connect_args(db_engine.url)\n",
    "    with db_engine.connect() as connection:\n",
    "        logger.info(f\"Connected to database using {db_engine}\")\n",
    "        yield connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename events to events_distributed\n",
    "\n",
    "db_params = get_clickhouse_params_from_env_vars()\n",
    "\n",
    "with get_clickhouse_connection(\n",
    "    **db_params,\n",
    ") as connection:\n",
    "    assert type(connection) == Connection\n",
    "\n",
    "    query = f\"SELECT database, name from system.tables\"\n",
    "    df = pd.read_sql(sql=query, con=connection)\n",
    "    display(df)\n",
    "\n",
    "    database = db_params[\"database\"]\n",
    "    xs = df.loc[(df.database == db_params[\"database\"]) & (df.name == \"events\")]\n",
    "    if xs.shape[0] > 0:\n",
    "        query = f\"RENAME TABLE {database}.events TO {database}.events_distributed\"\n",
    "        ys = pd.read_sql(sql=query, con=connection)\n",
    "        display(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def fillna(s: Optional[Any]) -> str:\n",
    "    quote = \"'\"\n",
    "#     return f\"{quote + '' + quote if (s is None) else quote + str(s) + quote}\"\n",
    "    return f\"{quote + ('' if s is None else str(s)) + quote}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff8918",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert fillna(\"\") == \"''\"\n",
    "assert fillna(\"Davor\") == \"'Davor'\"\n",
    "assert fillna(None) == \"''\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_duplicated_test_ddf():\n",
    "    df = pd.DataFrame(\n",
    "        dict(\n",
    "            AccountId=12345,\n",
    "            PersonId=[1, 2, 2, 3, 3, 3],\n",
    "            OccurredTime=[\n",
    "                datetime.fromisoformat(\n",
    "                    f\"2023-07-10T13:27:{i:02d}.{123456*(i+1) % 1_000_000:06d}\"\n",
    "                )\n",
    "                for i in range(6)\n",
    "            ],\n",
    "            DefinitionId=[\"one\"] * 3 + [\"two\"] * 2 + [\"three\"],\n",
    "            ApplicationId = None,\n",
    "        )\n",
    "    )\n",
    "    df[\"OccurredTimeTicks\"] = df[\"OccurredTime\"].astype(int) // 1000\n",
    "    df = pd.concat([df]*3)\n",
    "    df = df.sort_values(list(df.columns))\n",
    "#     df = df.set_index(\"PersonId\")\n",
    "    return dd.from_pandas(df, npartitions=2)\n",
    "\n",
    "ddf = create_duplicated_test_ddf()\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c9a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _pandas2dask_map(df: pd.DataFrame, *, history_size: Optional[int] = None) -> pd.DataFrame:\n",
    "    df = df.reset_index()\n",
    "    df = df.sort_values([\"PersonId\", \"OccurredTime\", \"OccurredTimeTicks\"])\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.set_index(\"PersonId\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = create_duplicated_test_ddf()\n",
    "df = ddf.compute().set_index(\"PersonId\")\n",
    "\n",
    "expected = pd.DataFrame({\n",
    "    \"AccountId\": [12345, 12345, 12345, 12345, 12345, 12345],\n",
    "    \"OccurredTime\": [\n",
    "        \"2023-07-10 13:27:00.123456\",\n",
    "        \"2023-07-10 13:27:01.246912\",\n",
    "        \"2023-07-10 13:27:02.370368\",\n",
    "        \"2023-07-10 13:27:03.493824\",\n",
    "        \"2023-07-10 13:27:04.617280\",\n",
    "        \"2023-07-10 13:27:05.740736\",\n",
    "    ],\n",
    "    \"DefinitionId\": [\"one\", \"one\", \"one\", \"two\", \"two\", \"three\"],\n",
    "    \"ApplicationId\": [None, None, None, None, None, None],\n",
    "    \"OccurredTimeTicks\": [\n",
    "        1688995620123456,\n",
    "        1688995621246912,\n",
    "        1688995622370368,\n",
    "        1688995623493824,\n",
    "        1688995624617280,\n",
    "        1688995625740736,\n",
    "    ],\n",
    "}, index=pd.Index([1, 2, 2, 3, 3, 3], name=\"PersonId\"))\n",
    "expected[\"OccurredTime\"] = pd.to_datetime(expected[\"OccurredTime\"])\n",
    "expected[\"DefinitionId\"] = expected[\"DefinitionId\"].astype(\"string[pyarrow]\")\n",
    "expected[\"ApplicationId\"] = expected[\"ApplicationId\"].astype(\"string[pyarrow]\")\n",
    "\n",
    "actual = _pandas2dask_map(df)\n",
    "\n",
    "pd.testing.assert_frame_equal(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8441062",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _pandas2dask(downloaded_path: Path, output_path: Path, *, history_size: Optional[int] = None) -> None:\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        d = Path(td)\n",
    "\n",
    "        ddf = dd.read_parquet(\n",
    "            downloaded_path,\n",
    "            blocksize=None,\n",
    "        )\n",
    "        ddf[\"AccountId\"] = ddf[\"AccountId\"].astype(\"int64\")\n",
    "        \n",
    "        # set index\n",
    "        ddf = ddf.set_index(\"PersonId\")\n",
    "        ddf.to_parquet(d, engine=\"pyarrow\")\n",
    "\n",
    "        # deduplicate and sort by PersonId and OccurredTime\n",
    "        ddf = dd.read_parquet(\n",
    "            d\n",
    "        )\n",
    "\n",
    "        ddf = ddf.map_partitions(_pandas2dask_map)\n",
    "\n",
    "        ddf.to_parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as td:\n",
    "    d = Path(td)\n",
    "    ddf = create_duplicated_test_ddf()\n",
    "    (d / \"duplicated\").mkdir()\n",
    "    for i, partition in enumerate(ddf.partitions):\n",
    "        partition.compute().to_parquet(d / \"duplicated\" / f\"part_{i:06d}.parquet\")\n",
    "\n",
    "    _pandas2dask(d / \"duplicated\", d / \"result\")\n",
    "\n",
    "    ddf = dd.read_parquet(d / \"result\")\n",
    "\n",
    "    display(ddf)\n",
    "    display(ddf.compute())\n",
    "\n",
    "    expected = pd.DataFrame({\n",
    "        \"AccountId\": [12345, 12345, 12345, 12345, 12345, 12345],\n",
    "        \"OccurredTime\": [\n",
    "            \"2023-07-10 13:27:00.123456\",\n",
    "            \"2023-07-10 13:27:01.246912\",\n",
    "            \"2023-07-10 13:27:02.370368\",\n",
    "            \"2023-07-10 13:27:03.493824\",\n",
    "            \"2023-07-10 13:27:04.617280\",\n",
    "            \"2023-07-10 13:27:05.740736\",\n",
    "        ],\n",
    "        \"DefinitionId\": [\"one\", \"one\", \"one\", \"two\", \"two\", \"three\"],\n",
    "        \"ApplicationId\": [None, None, None, None, None, None],\n",
    "        \"OccurredTimeTicks\": [\n",
    "            1688995620123456,\n",
    "            1688995621246912,\n",
    "            1688995622370368,\n",
    "            1688995623493824,\n",
    "            1688995624617280,\n",
    "            1688995625740736,\n",
    "        ],\n",
    "    }, index=pd.Index([1, 2, 2, 3, 3, 3], name=\"PersonId\"))\n",
    "    expected[\"OccurredTime\"] = pd.to_datetime(expected[\"OccurredTime\"])\n",
    "    expected[\"DefinitionId\"] = expected[\"DefinitionId\"].astype(\"string[pyarrow]\")\n",
    "    expected[\"ApplicationId\"] = expected[\"ApplicationId\"].astype(\"string[pyarrow]\")\n",
    "\n",
    "    pd.testing.assert_frame_equal(ddf.compute(), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffa8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _download_account_id_rows_as_parquet(\n",
    "    *,\n",
    "    account_id: Union[int, str],\n",
    "    application_id: Optional[str],\n",
    "    history_size: Optional[int] = None,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    username: str,\n",
    "    password: str,\n",
    "    database: str,\n",
    "    protocol: str,\n",
    "    table: str,\n",
    "    chunksize: Optional[int] = 1_000_000,\n",
    "    index_column: str = \"PersonId\",\n",
    "    output_path: Path,\n",
    ") -> None:\n",
    "\n",
    "    with get_clickhouse_connection(  # type: ignore\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        table=table,\n",
    "        protocol=protocol,\n",
    "    ) as connection:\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            d = Path(td)\n",
    "            i = 0\n",
    "\n",
    "            query = f\"SELECT DISTINCT * FROM {table} WHERE AccountId={account_id}\" # nosec B608\n",
    "            if application_id is not None and application_id != \"\":\n",
    "                 query = query + f\" AND ApplicationId='{application_id}'\"\n",
    "            query = query + \" ORDER BY PersonId ASC, OccurredTimeTicks DESC\"\n",
    "            if history_size:\n",
    "                query = query + f\" LIMIT {history_size} BY PersonId\"\n",
    "    \n",
    "            logger.info(f\"_download_account_id_rows_as_parquet(): {query=}\")\n",
    "\n",
    "            (d / \"downloaded\").mkdir(parents=True, exist_ok=True)\n",
    "            for df in pd.read_sql(sql=query, con=connection, chunksize=chunksize):\n",
    "                fname = d / \"downloaded\" / f\"clickhouse_data_{i:09d}.parquet\"\n",
    "                logger.info(\n",
    "                    f\"_download_account_id_rows_as_parquet() Writing data retrieved from the database to temporary file: {fname}\"\n",
    "                )\n",
    "                df.to_parquet(fname, engine=\"pyarrow\")  # type: ignore\n",
    "                i = i + 1\n",
    "                \n",
    "            logger.info(\n",
    "                f\"_download_account_id_rows_as_parquet() Rewriting temporary parquet files from {d / f'clickhouse_data_*.parquet'} to output directory {output_path}\"\n",
    "            )\n",
    "            _pandas2dask(d / \"downloaded\", output_path)\n",
    "                        \n",
    "            # test if everything is ok\n",
    "            test_ddf = dd.read_parquet(output_path).head()           \n",
    "           \n",
    "           \n",
    "def download_account_id_rows_as_parquet(\n",
    "    *,\n",
    "    account_id: Union[int, str],\n",
    "    application_id: Optional[str],\n",
    "    history_size: Optional[int] = None,\n",
    "    chunksize: Optional[int] = 1_000_000,\n",
    "    index_column: str = \"PersonId\",\n",
    "    output_path: Path,\n",
    ") -> None:\n",
    "    \n",
    "    db_params = get_clickhouse_params_from_env_vars()\n",
    "    \n",
    "    return _download_account_id_rows_as_parquet(\n",
    "        account_id=account_id,\n",
    "        application_id=application_id,\n",
    "        history_size=history_size,\n",
    "        chunksize=chunksize,\n",
    "        index_column=index_column,\n",
    "        output_path=output_path,\n",
    "        **db_params, # type: ignore\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaed32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "\n",
    "AccountId=12344\n",
    "ModelId=20062\n",
    "\n",
    "for ApplicationId in [None, \"\", \"A1F7EDD6E6BA23EBCD167C9C986ACFCB\"]:\n",
    "    print(\"*\"*120)\n",
    "    print()\n",
    "    print(f\"{ApplicationId=}\")\n",
    "    print()\n",
    "    \n",
    "    output_path = Path(tempfile.mkdtemp(prefix=\"clickhouse_download_\"))\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    download_account_id_rows_as_parquet(\n",
    "        account_id=AccountId,\n",
    "        application_id=ApplicationId,\n",
    "        output_path=output_path,\n",
    "    )\n",
    "\n",
    "    ddf = dd.read_parquet(output_path)\n",
    "    display(ddf.head())\n",
    "    print(f\"{ddf.shape[0].compute()=:,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bafc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "\n",
    "AccountId=12344\n",
    "# ModelId=10037\n",
    "for ApplicationId in [None, \"\", \"A1F7EDD6E6BA23EBCD167C9C986ACFCB\"]:\n",
    "    print(\"*\"*120)\n",
    "    print()\n",
    "    print(f\"{ApplicationId=}\")\n",
    "    print()\n",
    "    \n",
    "    output_path = Path(tempfile.mkdtemp(prefix=\"clickhouse_download_\"))\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    download_account_id_rows_as_parquet(\n",
    "        account_id=AccountId,\n",
    "        application_id=ApplicationId,\n",
    "        output_path=output_path,\n",
    "        history_size=30,\n",
    "    )\n",
    "\n",
    "    ddf = dd.read_parquet(output_path)\n",
    "    display(ddf.head(31))\n",
    "    display(ddf.tail(31))\n",
    "    print(f\"{ddf.shape[0].compute()=:,d}\")\n",
    "        \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "\n",
    "ddf = dd.read_parquet(\"/tmp/clickhouse_download_72car5_j\")\n",
    "display(ddf.loc[14438].compute().head(30))\n",
    "display(ddf.loc[14438].compute().tail(30))\n",
    "\n",
    "ddf = dd.read_parquet(\"/tmp/clickhouse_download_gyrb53v5\")\n",
    "display(ddf.loc[14438].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bf826",
   "metadata": {},
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ef6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def add_download_training_data(\n",
    "    app: FastKafka,\n",
    "    *,\n",
    "    root_path: Path,\n",
    "    username: str = \"infobip\",\n",
    ") -> None:\n",
    "    root_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    @app.produces(topic=f\"{username}_training_model_status\")  # type: ignore\n",
    "    async def to_training_model_status(\n",
    "        training_model_status: TrainingModelStatus,\n",
    "    ) -> TrainingModelStatus:\n",
    "        print(f\"to_training_model_status({training_model_status})\")\n",
    "        return training_model_status\n",
    "\n",
    "    @app.consumes(topic=f\"{username}_training_model_start\")  # type: ignore\n",
    "    async def on_training_model_start(\n",
    "        msg: TrainingModelStart, app: FastKafka = app\n",
    "    ) -> None:\n",
    "        try:\n",
    "            print(f\"on_training_model_start({msg}) started\")\n",
    "\n",
    "            AccountId = msg.AccountId\n",
    "            ApplicationId = msg.ApplicationId\n",
    "            ModelId = msg.ModelId\n",
    "            task_type = msg.task_type\n",
    "\n",
    "            dt = datetime.now().date().isoformat()\n",
    "            path = root_path / f\"AccountId-{AccountId}\" / f\"ApplicationId-{ApplicationId}\" / f\"ModelId-{ModelId}\" / dt\n",
    "            \n",
    "            \n",
    "            training_model_status = TrainingModelStatus(\n",
    "                AccountId=AccountId,\n",
    "                ApplicationId=ApplicationId,\n",
    "                ModelId=ModelId,\n",
    "                current_step=0,\n",
    "                current_step_percentage=0.0,\n",
    "                total_no_of_steps=3,\n",
    "            )\n",
    "            await app.to_training_model_status(training_model_status)\n",
    "\n",
    "            if path.exists():\n",
    "                print(\n",
    "                    f\"on_training_model_start({msg}): path '{path}' exists, moving on...\"\n",
    "                )\n",
    "            else:\n",
    "                # this mean we can download data from clickhouse\n",
    "\n",
    "                path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                print(f\"on_training_model_start({msg}): downloading data to '{path}'...\")\n",
    "                with using_cluster(\"cpu\"):\n",
    "                    download_account_id_rows_as_parquet(\n",
    "                        account_id=AccountId,\n",
    "                        application_id=ApplicationId,\n",
    "                        output_path=path,\n",
    "                    )\n",
    "\n",
    "                print(f\"on_training_model_start({msg}): data downloaded to '{path}'...\")\n",
    "\n",
    "            training_model_status = TrainingModelStatus(\n",
    "                AccountId=AccountId,\n",
    "                ApplicationId=ApplicationId,\n",
    "                ModelId=ModelId,\n",
    "                current_step=0,\n",
    "                current_step_percentage=1.0,\n",
    "                total_no_of_steps=3,\n",
    "            )\n",
    "            await app.to_training_model_status(training_model_status)\n",
    "\n",
    "        finally:\n",
    "            print(f\"on_training_model_start({msg}) finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with monkeypatch_clickhouse_params_from_env_vars():\n",
    "\n",
    "kafka_brokers = dict(\n",
    "    localhost={\n",
    "        \"url\": \"localhost\",\n",
    "        \"port\": 9092,\n",
    "    },\n",
    "    staging={\n",
    "        \"url\": environ[\"KAFKA_HOSTNAME\"],\n",
    "        \"port\": environ[\"KAFKA_PORT\"],\n",
    "        \"description\": \"Staging Kafka broker\",\n",
    "        \"protocol\": \"kafka-secure\",\n",
    "        \"security\": {\"type\": \"scramSha256\"},\n",
    "    }\n",
    ")\n",
    "app = FastKafka(kafka_brokers=kafka_brokers)\n",
    "\n",
    "with TemporaryDirectory(prefix=\"infobip_downloader_\") as d:\n",
    "    root_path = Path(d)\n",
    "\n",
    "    add_download_training_data(app, root_path=root_path)\n",
    "\n",
    "    tester = Tester(app)\n",
    "\n",
    "    async with tester:\n",
    "        AccountId = 317238\n",
    "        ModelId = \"10051\"\n",
    "        ApplicationId = \"MobileApp\"\n",
    "\n",
    "        training_model_start = TrainingModelStart(\n",
    "            AccountId=AccountId,\n",
    "            ApplicationId=ApplicationId,\n",
    "            ModelId=ModelId,\n",
    "            no_of_records=1_000,\n",
    "            task_type=\"churn\",\n",
    "        )\n",
    "\n",
    "        heading(f\"tester.to_infobip_training_model_start({training_model_start})\")\n",
    "\n",
    "        await tester.to_infobip_training_model_start(training_model_start)\n",
    "\n",
    "        heading(\n",
    "            f\"tester.awaited_mocks.on_infobip_training_model_status.assert_called()\"\n",
    "        )\n",
    "\n",
    "        await tester.awaited_mocks.on_infobip_training_model_status.assert_called(\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        dt = datetime.now().date().isoformat()\n",
    "        data_path = (\n",
    "            root_path\n",
    "            / f\"AccountId-{AccountId}\"\n",
    "            / f\"ApplicationId-{ApplicationId}\"\n",
    "            / f\"ModelId-{ModelId}\"\n",
    "            / dt\n",
    "            / \"part.0.parquet\"\n",
    "        )\n",
    "        assert data_path.exists(), data_path\n",
    "\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def create_app(*, root_path: Optional[Path]=None, group_id: Optional[str] = None) -> FastKafka:\n",
    "    if group_id is None:\n",
    "        group_id = f\"infobip-downloader-{random.randint(100_000_000, 999_999_999):0,d}\".replace( # nosec: B311:blacklist\n",
    "            \",\", \"-\"\n",
    "        )\n",
    "        \n",
    "    print(f\"{group_id=}\")\n",
    "    if root_path is None:\n",
    "        root_path = Path(\".\") / group_id\n",
    "\n",
    "    app = create_fastkafka_application(group_id=group_id)\n",
    "\n",
    "    add_logging(app)\n",
    "    add_download_training_data(app, root_path=root_path)\n",
    "    \n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb80931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "\n",
    "\n",
    "app = create_app(group_id=downloading_group_id)\n",
    "\n",
    "app.set_kafka_broker(\"staging\")\n",
    "\n",
    "async with app:\n",
    "    while(True):\n",
    "        await asyncio.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
